# example fluentd configuration file

# Don't ship fluentd's own log to Kinesis. Fun blow-ups can occur
# (like it attempting to emit errors about too long messages that fail
# with the entire too long message in it...)
# Note that even though we set type null, the fluentd log still goes
# to stdout.
<match fluent.**>
  type null
</match>

<source>
  type forward
</source>

<source>
  type monitor_agent
</source>

<source>
  type debug_agent
</source>

# Tail some arbitrary file.
<source>
  type tail
  path /fluentd/log/file.log
  pos_file /fluentd/log/file.log.pos
  tag your_awesome_logs

  # An example of parsing JSON loglines, with a fallback
  # if the JSON is unable to be parsed.
  format multi_format
  <pattern>
    format json
    time_key time
  </pattern>
  <pattern>
    format none
  </pattern>
</source>

<filter **>
  type record_transformer
  <record>
    # LaaS requirements
    serviceId <YOUR LAAS SERVICE ID>
    environment <YOUR LAAS ENVIRONMENT>
  </record>
</filter>

<match **>
  type kinesis

  region us-west-1

  # Either use role_arn, or keys (prefer role if possible)
  # role_arn <YOUR AWS ROLE>
  aws_key_id <YOUR AWS KEY>
  aws_sec_key <YOUR AWS SECRET>

  stream_name logs
  # Restricts logs to a single shard, but helps make sure
  # loglines appear in order. If you are shipping a _lot_ of
  # logs, you may want to use 'random_partition_key true' instead,
  # and go multi-thread/process.
  partition_key_expr record['serviceId'] + '-' + record['environment']

  # Don't break on non-ascii/multi-byte chars (i.e. utf-8).
  use_yajl true

  # This is actually the default config, but it seems better to
  # make it explicit.
  time_key time
  include_time_key true
  tag_key tag
  include_tag_key true

  # Note that retries are handled by kinesis plugin so that
  # it can handle some record failures (i.e. when not all records fail).
  # If it's still failing after its internal retries fail, it
  # throws away the chunk _without_ causing an error, so fluentd
  # proper will not retry. Unfortunately, there's no way to specify
  # an equivalent of max_retry_wait (keeps exponentially backing off),
  # so if we set the retries too high it can take a long time to
  # come back after an outage.
  # These 12 retries will go for approx 2 hours, based on this delay:
  #    2^n * 0.5
  # This means that, once the stream becomes unthrottled/responsive
  # again, it can take us up to ~ 1 hour to try again.
  retries_on_putrecords 13 # implies 13 calls - i.e. 12 retries

  # If we fail talking to Kinesis completely though (i.e. some
  # API exception), we do want fluentd to retry... though I imagine
  # there is the danger of resubmitting log records since the
  # whole chunk will be retried.
  retry_wait 1s
  max_retry_wait 300s # ~5 minutes
  retry_limit 300 # ~24 hours max time to retry

  # no memory, because 1gb buffer and fluentd recommend not using
  # memory (file is actually faster, apparently). Also, good if
  # fluentd is forced to restart.
  buffer_type file
  buffer_path /fluentd/log/fluentd.*.buffer
  # If the box is going down, we're probably not going to see our disk again,
  # so flush if we can.
  flush_at_shutdown true
  # We use 1gb at most here (worst case scenario).
  buffer_queue_limit 111
  buffer_chunk_limit 9m # maximum kinesis putRecords is 5mb anyway
  # Ship reasonably frequently so people see logs ASAP.
  flush_interval 5s
</match>
